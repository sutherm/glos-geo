{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947d14f2-f0ce-4ede-9d75-82b106be4b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import pdal\n",
    "import json\n",
    "from glob import glob\n",
    "import os\n",
    "import h3\n",
    "from shapely.geometry import Polygon\n",
    "import geopandas as gpd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c5cb77-93a6-451e-8f55-d7f46d9920a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "\n",
    "def parse_coords(file_path, reader_type):\n",
    "    \"\"\"\n",
    "    Reads a .las/z or ascii xyz file using PDAL andreturns arrays containing latitude and longitude values\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the data file\n",
    "    reader_type (str): the PDAL reader appropriate for the file; in this case use \"las\" or \"text\"\n",
    "    \n",
    "    Returns:\n",
    "    two arrays containing lat and lon values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the PDAL pipeline to read the .laz file\n",
    "    pipeline = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": f\"readers.{reader_type}\",\n",
    "                \"filename\": file_path\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create PDAL pipeline manager\n",
    "    pipeline_manager = pdal.Pipeline(json.dumps(pipeline))\n",
    "\n",
    "    # Execute the pipeline\n",
    "    pipeline_manager.execute()\n",
    "\n",
    "    # Fetch the numpy array containing point cloud data\n",
    "    arrays = pipeline_manager.arrays\n",
    "    \n",
    "    # Extract x, y coordinates from the point cloud\n",
    "    if reader_type == \"las\":\n",
    "        lats = arrays[0]['Y']\n",
    "        lons = arrays[0]['X']\n",
    "        \n",
    "    elif reader_type == \"text\":\n",
    "        lats = arrays[0]['LATITUDE']\n",
    "        lons = arrays[0]['LONGITUDE']\n",
    "    \n",
    "    return lats, lons\n",
    "\n",
    "def create_h3_hex(h3_res, lats, lons):\n",
    "    \"\"\"\n",
    "    Creates WKT geometry representing the h3 hexagons corresponding to spatial extent of input data file  \n",
    "    \n",
    "    Parameters:\n",
    "    h3_res (str): Desired H3 level\n",
    "    lats (str): array of latitude coordinates\n",
    "    lons (str): array of longitude coordinates\n",
    "    \n",
    "    Returns:\n",
    "    List of WKT geometries \n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the latitude and longitude arrays to H3 indexes\n",
    "    h3_indices = set(h3.geo_to_h3(lat, lon, h3_res) for lat, lon in zip(lats, lons))\n",
    "    \n",
    "    # Create hexagon geometry by converting H3 indices to polygons\n",
    "    h3_polygons = [Polygon(h3.h3_to_geo_boundary(h, geo_json=True)) for h in h3_indices]\n",
    "    \n",
    "    return h3_polygons\n",
    "\n",
    "def gdf2poly(h3_polys, surveyid, outfile):\n",
    "    # Convert H3 polygons to a geodataframe, converting to a vector file format and write to disk\n",
    "    gdf = gpd.GeoDataFrame({'survey_id': surveyid, 'geometry': h3_polys}, crs = \"EPSG:4326\")\n",
    "    gdf.to_file(outfile)\n",
    "    \n",
    "def json_extract_platformID_date(jsonfile):    \n",
    "    with open(jsonfile, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        platform_info = data['platform']\n",
    "        temporal_info = data['submissionInfo']\n",
    "        \n",
    "    return platform_info['uniqueID'], temporal_info['timeCode']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ede9b-848c-4b49-8932-0b319f164400",
   "metadata": {},
   "source": [
    "### Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500cebb2-4d67-417a-ac01-cf8fe944826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User definition of variables\n",
    "\n",
    "inpath = r\"/mnt/c/Users/mike/glos_data/scratch\"\n",
    "outpath = inpath\n",
    "survname = \"test\"\n",
    "lof = glob(os.path.join(inpath, \"*.xyz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b66010-d275-455d-82f8-345e9d37ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through list of files defined above and create H3 hexagon outputs\n",
    "\n",
    "for f in lof:\n",
    "    print(f\"processing file {f}\") \n",
    "    coords = parse_coords(f, \"text\")\n",
    "    \n",
    "    for hex_level in range(6, 11):\n",
    "        output = os.path.join(outpath, f\"{survname}_h3-{str(hex_level)}.geojson\")\n",
    "        h3_hex = create_h3_hex(hex_level, coords[0], coords[1])\n",
    "        hexpoly = gdf2poly(h3_hex, survname, output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324defc8-84c3-435a-8e22-84a189be7f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate multiple H3 hexagon vectors (surveys consisting of multiple files) and remove duplicative records\n",
    "# A final aggregated vector file is written to disk\n",
    "\n",
    "for i in range(6,11):\n",
    "    print(i)\n",
    "    \n",
    "    gjson_lof = glob(os.path.join(outpath, f\"*{i}.geojson\"))\n",
    "    gjson_gdf = []\n",
    "\n",
    "    # Read each geojson file into a geodataframe\n",
    "    for j in gjson_lof:\n",
    "        gdf = gpd.read_file(j)\n",
    "        gjson_gdf.append(gdf)\n",
    "    \n",
    "    # Create aggregated geodataframe\n",
    "    comb_gdf = gpd.GeoDataFrame(pd.concat(gjson_gdf, ignore_index=True))\n",
    "\n",
    "    # Deduplicate based on all columns\n",
    "    deduplicated_gdf = comb_gdf.drop_duplicates().reset_index(drop=True)\n",
    "   \n",
    "    # Write to disk\n",
    "    formatted_value = f\"0{i}\" if i < 10 else str(i) \n",
    "    deduplicated_gdf.to_file(os.path.join(outpath,\"level\" + formatted_value + \"_dedup.geojson\"))\n",
    "    deduplicated_gdf.to_file(os.path.join(outpath,\"level\" + formatted_value + \"_dedup.shp\"))\n",
    "    gjson_gdf = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7307a3-783d-456b-b723-185afb6b4a75",
   "metadata": {},
   "source": [
    "#### optional processing required to query CSB metadata and pull distinctive attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235958aa-248a-474d-a43e-8b31afec70d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User definition of variables\n",
    "\n",
    "inpath = r\"/mnt/c/Users/mike/glos_data/ncei_csb/datafiles/\"\n",
    "lod = os.listdir(inpath)\n",
    "lod[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e1282-a8f3-475c-9bd5-cdb386abb2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lod:\n",
    "    jsonf = os.path.join(inpath, i, \"metadata.json\")\n",
    "    surveyf = os.path.join(inpath, i, \"formatted\", \"data.xyz\")\n",
    "    metadata = json_extract_platformID_date(jsonf)\n",
    "    coords = parse_coords(surveyf)\n",
    "    \n",
    "    for hex_level in range(6, 11):\n",
    "        survname = \"IHO DCDB Crowd-Sourced Bathymetry\"\n",
    "        output = os.path.join(inpath, \"geojson\", f\"{i}_h3-\" + str(hex_level) + \".geojson\")\n",
    "        h3_hex = create_h3_hex(hex_level, coords[0], coords[1])\n",
    "        hexpoly = gdf2poly(h3_hex, survname, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
